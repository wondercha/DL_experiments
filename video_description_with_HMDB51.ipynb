{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video_description_with_HMDB51.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHMh5D3slN5OtdRnQFRcn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wondercha/DL_experiments/blob/main/video_description_with_HMDB51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txtBPQD1L9BD"
      },
      "source": [
        "# Install dependencies for video module in pytorch to work\n",
        "! apt-get install libavformat-dev libavdevice-dev\n",
        "! pip install av==6.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkNPB68MLX1"
      },
      "source": [
        "# Download HMDB51 data and splits from serre lab website\n",
        "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
        "! wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/test_train_splits.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clGTGIJdNYbK",
        "outputId": "26d82cdb-1cce-4160-aed0-5f78107c06a8"
      },
      "source": [
        "# Import required modules ...\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision\n",
        "from torchvision import get_video_backend\n",
        "from torchvision.models.video import r3d_18 \n",
        "from torchvision import transforms\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "import av\n",
        "import random\n",
        "print(f\"PyAV version -- {av.__version__}\")\n",
        "\n",
        "SEED = 491\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "from collections import OrderedDict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#run_av_diagnostics()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyAV version -- 6.2.0\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbhkaaM-N0-r"
      },
      "source": [
        "# Extract and organize video data..\n",
        "! mkdir -p video_data test_train_splits\n",
        "! unrar e test_train_splits.rar test_train_splits\n",
        "! rm test_train_splits.rar\n",
        "! unrar e hmdb51_org.rar \n",
        "! rm hmdb51_org.rar\n",
        "! mv *.rar video_data\n",
        "for files in os.listdir('video_data'):\n",
        "    foldername = files.split('.')[0]\n",
        "    os.system(\"mkdir -p video_data/\" + foldername)\n",
        "    os.system(\"unrar e video_data/\"+ files + \" video_data/\"+foldername)\n",
        "\n",
        "! rm video_data/*.rar "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw8ak4-rOLWN",
        "outputId": "679c8aac-ae18-4581-baba-be3e5ed9d6b9"
      },
      "source": [
        "# diagnostics for PyAV installation. For now version 6.2.0 works \n",
        "def run_av_diagnostics():\n",
        "    import av\n",
        "    av.open(\"video_data/brush_hair/Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ba_goo_4.avi\")\n",
        "    print(get_video_backend())\n",
        "    av.logging.set_level(av.logging.ERROR)\n",
        "    if not hasattr(av.video.frame.VideoFrame, 'pict_type'):\n",
        "      print(\"Nah\")\n",
        "\n",
        "run_av_diagnostics()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5uEVSv7OPr-",
        "outputId": "6e7dd94a-446b-416c-b187-af0f8b4ab2cc"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=f36e61a58ba6ca3bad07ce47d46335e7c9d730de5350e9d98478846af6abdb68\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  |     Proc size: 328.4 MB\n",
            "GPU RAM Free: 11438MB | Used: 3MB | Util   0% | Total     11441MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-GRhxPjOVjr",
        "outputId": "ab6ca24b-2c79-41ae-c570-0f29c45568db"
      },
      "source": [
        "# download video transformation functions from below link\n",
        "! wget https://raw.githubusercontent.com/pytorch/vision/6de158c473b83cf43344a0651d7c01128c7850e6/references/video_classification/transforms.py\n",
        "import transforms as T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 01:15:14--  https://raw.githubusercontent.com/pytorch/vision/6de158c473b83cf43344a0651d7c01128c7850e6/references/video_classification/transforms.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3102 (3.0K) [text/plain]\n",
            "Saving to: ‘transforms.py’\n",
            "\n",
            "\rtransforms.py         0%[                    ]       0  --.-KB/s               \rtransforms.py       100%[===================>]   3.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-03 01:15:15 (32.7 MB/s) - ‘transforms.py’ saved [3102/3102]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh7qQJ7aOU6b"
      },
      "source": [
        "# Datasets and Dataloaders for model training ..\n",
        "\n",
        "val_split = 0.05\n",
        "num_frames = 16 # 16\n",
        "clip_steps = 50\n",
        "num_workers = 8\n",
        "pin_memory = True\n",
        "train_tfms = torchvision.transforms.Compose([\n",
        "                                 T.ToFloatTensorInZeroOne(),\n",
        "                                 T.Resize((128, 171)),\n",
        "                                 T.RandomHorizontalFlip(),\n",
        "                                 T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "                                 T.RandomCrop((112, 112))\n",
        "                               ])  \n",
        "test_tfms =  torchvision.transforms.Compose([\n",
        "                                             T.ToFloatTensorInZeroOne(),\n",
        "                                             T.Resize((128, 171)),\n",
        "                                             T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "                                             T.CenterCrop((112, 112))\n",
        "                                             ])\n",
        "hmdb51_train = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n",
        "                                                step_between_clips = clip_steps, fold=1, train=True,\n",
        "                                                transform=train_tfms, num_workers=num_workers)\n",
        "\n",
        "\n",
        "hmdb51_test = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n",
        "                                                step_between_clips = clip_steps, fold=1, train=False,\n",
        "                                                transform=test_tfms, num_workers=num_workers)\n",
        "      \n",
        "total_train_samples = len(hmdb51_train)\n",
        "total_val_samples = round(val_split * total_train_samples)\n",
        "\n",
        "print(f\"number of train samples {total_train_samples}\")\n",
        "print(f\"number of validation samples {total_val_samples}\")\n",
        "print(f\"number of test samples {len(hmdb51_test)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3h5AUhgOXc1",
        "outputId": "7558f49d-cf3f-422f-b24f-32437f7e733b"
      },
      "source": [
        "# set up a sample pytorch class to check if it works\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Model, self).__init__()\n",
        "      self.base_model = nn.Sequential(*list(r3d_18(pretrained=False).children())[:-1])\n",
        "      self.fc1 = nn.Linear(512, 51)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.base_model(x).squeeze(4).squeeze(3).squeeze(2)\n",
        "      print(\"size after pretrained model \", out.size())\n",
        "      out = torch.log_softmax(self.fc1(out), dim=1)\n",
        "      return out\n",
        "\n",
        "check = Model().cuda()\n",
        "out = check(torch.randn(16, 3 , 8, 112, 112).cuda())\n",
        "out.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size after pretrained model  torch.Size([16, 512])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 51])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmuC3KNeOfLz"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def update_acc(self, val, n=1):\n",
        "        self.val = val/n\n",
        "        self.sum += val\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otqihl1GOiBs"
      },
      "source": [
        "# define model using PyTorch\n",
        "class VideoRecog_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(VideoRecog_Model, self).__init__()\n",
        "      self.base_model = nn.Sequential(*list(r3d_18(pretrained=True).children())[:-1])\n",
        "      self.fc1 = nn.Linear(512, 51)\n",
        "      self.fc2 = nn.Linear(51, 51) \n",
        "      self.dropout = nn.Dropout2d(0.5) \n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.base_model(x).squeeze(4).squeeze(3).squeeze(2) # output of base model is bs x 512 x 1 x 1 x 1\n",
        "      out = F.relu(self.fc1(out)) \n",
        "      out = self.dropout(out) \n",
        "      out = torch.log_softmax(self.fc2(out), dim=1)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILtnH8kJTaF8"
      },
      "source": [
        "# define model using pre-trained resnet101\n",
        "import torchvision.models as models\n",
        "\n",
        "class VideoClass_Model(nn.Module):\n",
        "  def __init__(self, num_classes=51):\n",
        "      super(VideoClass_Model, self).__init__()\n",
        "      resnet = models.resnet101(pretrained=True)\n",
        "      modules = list(resnet.children())[:-1]  # remove the last FC layer\n",
        "      self.resnet = nn.Sequential(*modules)\n",
        "      self.linear = nn.Linear(resnet.fc.in_features, num_classes)\n",
        "      self.bn = nn.BatchNorm1d(num_classes, momentum=0.01)\n",
        "\n",
        "  def forward(self, x):\n",
        "      with torch.no_grad(): # 네트워크의 앞 부분은 변경되지 않도록 하기\n",
        "          out = self.resnet(x)\n",
        "      #print('size:', out.size())\n",
        "      out = out.reshape(out.size(0), -1)\n",
        "      out = self.bn(self.linear(out))\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNZl4T3LQVwn",
        "outputId": "f43eaaa9-7db2-49fc-dbd1-2e42958fe671"
      },
      "source": [
        "check = VideoClass_Model().cuda()\n",
        "out = check(torch.randn(64,3,7,7).cuda())\n",
        "out.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 51])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t51S5bdCOjDb"
      },
      "source": [
        "def train(config, model, loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    config = {}\n",
        "    config['log_interval'] = 100\n",
        "    correct = 0\n",
        "    total_loss = 0.0\n",
        "    flag = 0\n",
        "    Loss, Acc = AverageMeter(), AverageMeter()\n",
        "    start = time.time()\n",
        "    for batch_id, data in enumerate(loader):\n",
        "        data, target = data[0], data[-1]\n",
        "        # print(\"here\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "           data = data.cuda()\n",
        "           target = target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        Loss.update(loss.item(), data.size(0))\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        num_corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "        correct += num_corrects\n",
        "\n",
        "        Acc.update_acc(num_corrects, data.size(0))\n",
        "\n",
        "        if flag!= 0 and batch_id%config['log_interval'] == 0:\n",
        "           print('Train Epoch: {} Batch [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {}/{} ({:.0f})%'.format(\n",
        "                epoch, batch_id * len(data), len(loader.dataset),\n",
        "                100. * batch_id / len(loader), Loss.avg, correct, Acc.count, 100. * Acc.avg))\n",
        "        flag = 1\n",
        "\n",
        "    #total_loss /= len(loader.dataset) \n",
        "    print('Train Epoch: {} Average Loss: {:.6f} Average Accuracy: {}/{} ({:.0f})%'.format(\n",
        "         epoch, Loss.avg, correct, Acc.count, 100. * Acc.avg ))\n",
        "    print(f\"Takes {time.time() - start}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq_gAdigOl6m"
      },
      "source": [
        "def test(config, model, loader, text='Validation'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total_loss = 0.0\n",
        "    Loss, Acc = AverageMeter(), AverageMeter()\n",
        "    with torch.no_grad():\n",
        "         for batch_id, data in enumerate(loader):\n",
        "             data, target = data[0], data[-1]\n",
        "\n",
        "             if torch.cuda.is_available():\n",
        "                data = data.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "             output = model(data)\n",
        "             loss = F.nll_loss(output, target)\n",
        "             total_loss += loss.item()\n",
        "\n",
        "             Loss.update(loss.item(), data.size(0))\n",
        "\n",
        "             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "             num_corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "             correct += num_corrects\n",
        "\n",
        "             Acc.update_acc(num_corrects, data.size(0))\n",
        "           \n",
        "    total_loss /= len(loader.dataset)\n",
        "    print(text + ' Average Loss: {:.6f} Average Accuracy: {}/{} ({:.0f})%'.format(\n",
        "         Loss.avg, correct, Acc.count , 100. * Acc.avg ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "ApalKayqO1FX",
        "outputId": "4eb9d206-62e4-42b7-a030-84468e505726"
      },
      "source": [
        "bs = 4\n",
        "lr = 1e-2\n",
        "gamma = 0.7\n",
        "total_epochs = 10\n",
        "config = {}\n",
        "num_workers = 0\n",
        "\n",
        "kwargs = {'num_workers':num_workers, 'pin_memory':True} if torch.cuda.is_available() else {'num_workers':num_workers}\n",
        "#kwargs = {'num_workers':num_workers}\n",
        "#kwargs = {}\n",
        "\n",
        "hmdb51_train_v1, hmdb51_val_v1 = random_split(hmdb51_train, [total_train_samples - total_val_samples,\n",
        "                                                                       total_val_samples])\n",
        "\n",
        "#hmdb51_train_v1.video_clips.compute_clips(16, 1, frame_rate=30)\n",
        "#hmdb51_val_v1.video_clips.compute_clips(16, 1, frame_rate=30)\n",
        "#hmdb51_test.video_clips.compute_clips(16, 1, frame_rate=30)\n",
        "\n",
        "#train_sampler = RandomClipSampler(hmdb51_train_v1.video_clips, 5)\n",
        "#test_sampler = UniformClipSampler(hmdb51_test.video_clips, 5)\n",
        "  \n",
        "train_loader = DataLoader(hmdb51_train_v1, batch_size=bs, shuffle=True, **kwargs)\n",
        "val_loader   = DataLoader(hmdb51_val_v1, batch_size=bs, shuffle=True, **kwargs)\n",
        "test_loader  = DataLoader(hmdb51_test, batch_size=bs, shuffle=False, **kwargs)\n",
        "\n",
        "model = VideoRecog_Model()\n",
        "#model = VideoClass_Model()\n",
        "model = torch.nn.DataParallel(model)\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   model = model.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "print(\"Launching Action Recognition Model training\")\n",
        "for epoch in range(1, total_epochs + 1):\n",
        "    train(config, model, train_loader, optimizer, epoch)\n",
        "    test(config, model, val_loader, text=\"Validation\")\n",
        "    scheduler.step()\n",
        "\n",
        "test(config, model, test_loader, text=\"Test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Action Recognition Model training\n",
            "Train Epoch: 1 Batch [400/7366 (5%)]\tLoss: 4.072005 Accuracy: 14/404 (3)%\n",
            "Train Epoch: 1 Batch [800/7366 (11%)]\tLoss: 3.988585 Accuracy: 28/804 (3)%\n",
            "Train Epoch: 1 Batch [1200/7366 (16%)]\tLoss: 3.957716 Accuracy: 47/1204 (4)%\n",
            "Train Epoch: 1 Batch [1600/7366 (22%)]\tLoss: 3.937696 Accuracy: 71/1604 (4)%\n",
            "Train Epoch: 1 Batch [2000/7366 (27%)]\tLoss: 3.928782 Accuracy: 89/2004 (4)%\n",
            "Train Epoch: 1 Batch [2400/7366 (33%)]\tLoss: 3.918591 Accuracy: 111/2404 (5)%\n",
            "Train Epoch: 1 Batch [2800/7366 (38%)]\tLoss: 3.909686 Accuracy: 131/2804 (5)%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d22dd4c6a624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching Action Recognition Model training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-27ddf99b1802>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, model, loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwsnuO8UPHur",
        "outputId": "474712ea-3cd6-402e-b837-0431e36c9033"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyO0Y6yAPJq9"
      },
      "source": [
        "for i in DataLoader(hmdb51_train, batch_size=4):\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}